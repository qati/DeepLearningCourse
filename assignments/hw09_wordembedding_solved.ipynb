{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = 'A corpus may contain texts in a single language (monolingual corpus) or text data in multiple languages (multilingual corpus). Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora. There are two main types of parallel corpora which contain texts in two languages. In a translation corpus, the texts in one language are translations of texts in the other language. In a comparable corpus, the texts are of the same kind and cover the same content, but they are not translations of each other. To exploit a parallel text, some kind of text alignment identifying equivalent text segments (phrases or sentences) is a prerequisite for analysis. Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus. In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation. An example of annotating a corpus is part-of-speech tagging, or POS-tagging, in which information about each word\\'s part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags. Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual.'\n",
    "example_sentece = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_non_alphanumeric_chars(input_text):\n",
    "    \"\"\"\n",
    "        Returns the non alphanumeric unique characters from a corpus.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * non_alphanumeric_unique: list of non alphanumeric characters\n",
    "    \"\"\"     \n",
    "    ###Start code here\n",
    "    \n",
    "    #STEP1 set character_list variable to a list of characters in the input_text \n",
    "    character_list = list(input_text)\n",
    "    \n",
    "    #STEP2 store all non_alphanumeric characters from the character_list!\n",
    "    non_alphanumeric = []\n",
    "    for character in character_list:\n",
    "        if not (character.isalnum()): non_alphanumeric.append(character)\n",
    "    \n",
    "    #STEP3 keep only the unique characters (hint: unique_values = list(set(duplicated_values))\n",
    "    non_alphanumeric_unique = list(set(non_alphanumeric))\n",
    "    \n",
    "    ###End code here \n",
    "    \n",
    "    return non_alphanumeric_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', ',', \"'\", '.', ')', '-', ' ']\n",
      "['/', '(', ')', '$', '=', ' ']\n"
     ]
    }
   ],
   "source": [
    "print(get_non_alphanumeric_chars(corpus))\n",
    "print(get_non_alphanumeric_chars('bla$$=()/ma z'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def tokenizer(input_text):\n",
    "    \"\"\"\n",
    "        Transforms strings to tokens. The tokens should appear in the same order as they are in the string.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * tokens: list of tokens\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    ###Start code here\n",
    "    #STEP0 extract non alphanumeric characters! Use your function above!\n",
    "    non_alphanumeric_unique = get_non_alphanumeric_chars(input_text)\n",
    "    \n",
    "    #STEP1 replace all non alphanumeric characters to ' ' whitespace.\n",
    "    replaced_input = input_text\n",
    "    for character in non_alphanumeric_unique:\n",
    "        replaced_input = replaced_input.replace(character,' ')\n",
    "    \n",
    "    #STEP2 split the input_text on all the non_alphanumeric_unique characters and store then in the tokens variable\n",
    "    tokens = replaced_input.split()\n",
    "    \n",
    "    ###End code here    \n",
    "    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monolingual', 'corpus', 'or', 'text', 'data', 'in', 'multiple', 'languages', 'multilingual', 'corpus', 'Multilingual', 'corpora', 'that']\n",
      "\n",
      "['Hi', 'How', 'are', 'you', 'I', 'm', 'fine', 'thanks', 'Bye']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(corpus[49:153]))\n",
    "print()\n",
    "print(tokenizer('Hi! How are you? I\\'m fine, thanks... Bye'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_dictionary(input_text):\n",
    "    \"\"\"\n",
    "        Builds up the dictionary from an input text.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * dictionary: list of unique, sorted tokens\n",
    "    \"\"\"    \n",
    "    tokens = tokenizer(input_text)\n",
    "    ###Start code here\n",
    "    \n",
    "    unique_tokens = list(set(tokens))\n",
    "    unique_sorted_tokens = sorted(unique_tokens)\n",
    "    \n",
    "    ###End code here\n",
    "    dictionary = unique_sorted_tokens\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'added', 'adjective', 'algorithms', 'aligned', 'alignment', 'an', 'analysis', 'and']\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "print(get_dictionary(corpus.lower())[0:10])\n",
    "print(len(get_dictionary(corpus.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def one_hot_encoded(word, dictionary):\n",
    "    \"\"\"\n",
    "        Turn a word to a one-hot encoded vector based on the dictionary.\n",
    "        Input: \n",
    "            * word:   string\n",
    "            * dictionary: list of words in a dictionary\n",
    "        Output:\n",
    "            * oh_vec: one-hot encoded vector\n",
    "    \"\"\"    \n",
    "    #check if word is in dictionary\n",
    "    if(not word in dictionary):\n",
    "        print('The word _' + word + '_ is not in dictionary!')\n",
    "        return None\n",
    "    \n",
    "    ###Start code here\n",
    "    # oh_vec should be a numpy array with a shape (len(dictionary), 1)\n",
    "    # it should contains 0 everywhere except for the position of the word in the dictionary. There is should be 1. \n",
    "    oh_vec = np.zeros((len(dictionary),1), dtype = int)\n",
    "    word_index = dictionary.index(word)\n",
    "    oh_vec[word_index] = 1\n",
    "    \n",
    "    ###End code here\n",
    "\n",
    "    return oh_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "\n",
      "The word _ablak_ is not in dictionary!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "\n",
    "print(one_hot_encoded('a', dic)[0:10])\n",
    "print()\n",
    "print(one_hot_encoded('aligned', dic)[0:10])\n",
    "print()\n",
    "print(one_hot_encoded('ablak', dic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def generate_CBOW_example(start_pos, half_window, tokens):\n",
    "    \"\"\"\n",
    "        Generated CBOW training examples from tokens\n",
    "        Input: \n",
    "            * start_pos:   the position of the first example\n",
    "            * half_window: number of tokens on one-side\n",
    "            * tokens: list of tokens\n",
    "        Output:\n",
    "            * X: input words (the neighbors)\n",
    "            * Y: target word\n",
    "    \"\"\"     \n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    X = []\n",
    "    for i in range(half_window):\n",
    "        X.append(tokens[i+start_pos])\n",
    "    for i in range(half_window):\n",
    "        X.append(tokens[i+half_window+start_pos+1])\n",
    "    \n",
    "    Y = tokens[start_pos + half_window]\n",
    "    \n",
    "    ###End code here\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'corpus', 'contain', 'texts'], 'may')\n",
      "(['may', 'contain', 'in', 'a'], 'texts')\n",
      "(['a', 'corpus', 'may', 'texts', 'in', 'a'], 'contain')\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "\n",
    "print(generate_CBOW_example(0, 2, corpus_tokens))\n",
    "print(generate_CBOW_example(2, 2, corpus_tokens))\n",
    "print(generate_CBOW_example(0, 3, corpus_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_skip_gram_example(start_pos, half_window, tokens):\n",
    "    \"\"\"\n",
    "        Generated skip-gram training examples from tokens\n",
    "        Input: \n",
    "            * start_pos:   the position of the first example\n",
    "            * half_window: number of tokens on one-side\n",
    "            * tokens: list of tokens\n",
    "        Output:\n",
    "            * x: input word\n",
    "            * y: target words\n",
    "    \"\"\"     \n",
    "    \n",
    "    X, Y = generate_CBOW_example(start_pos, half_window, tokens)\n",
    "    x = Y\n",
    "    y = X\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_keras_cbow(hidden_dim, dictionary_size):\n",
    "    \"\"\"\n",
    "        Generate keras model for CBOW model\n",
    "        Input: \n",
    "            * hidden_dim: number of nurons in the hidden layer\n",
    "            * dictionary_size: length of the dictionary (output/input size)\n",
    "        Output:\n",
    "            * model: keras model that implements CBOW \n",
    "    \"\"\"\n",
    "    cbow = Sequential()\n",
    "    ###Start code here\n",
    "    \n",
    "    # add a Dense layer with hidden_dim neurons. Input_dim is the dictionary_size \n",
    "    # we don't need nor activation neither bias!\n",
    "    cbow.add(Dense(hidden_dim, input_shape = (dictionary_size,)))\n",
    "    \n",
    "    # add a Dense layer with dictionary_size neurons. We don't need bias. Activation is softmax.\n",
    "    cbow.add(Dense(dictionary_size))\n",
    "    \n",
    "    ###End code here\n",
    "    \n",
    "    return cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "cbow_model = get_keras_cbow(30, len(dic))\n",
    "cbow_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 , Loss: 10.0020680376\n",
      "Epoch 02 , Loss: 10.4397078571\n",
      "Epoch 03 , Loss: 10.3292779068\n",
      "Epoch 04 , Loss: 10.5370182634\n",
      "Epoch 05 , Loss: 9.94345681059\n",
      "Epoch 06 , Loss: 10.8442729204\n",
      "Epoch 07 , Loss: 9.78638074368\n",
      "Epoch 08 , Loss: 10.7574621848\n",
      "Epoch 09 , Loss: 10.2302648257\n",
      "Epoch 10 , Loss: 10.6706129949\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "half_window = 2\n",
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "max_pos_start = len(corpus_tokens)-2*half_window\n",
    "\n",
    "for iteration in range(epochs): #iterate the corpus epochs times\n",
    "    loss = 0.\n",
    "    \n",
    "    for pos_start in range(max_pos_start): # iterate on the token positions\n",
    "        x, y = generate_CBOW_example(pos_start, half_window, corpus_tokens) # generate training examples\n",
    "        \n",
    "        x = np.array([one_hot_encoded(i, dic) for i in x]).sum(0).T \n",
    "        y = one_hot_encoded(y, dic).T\n",
    "        \n",
    "        loss += cbow_model.train_on_batch(x, y) # train the model. only 1 sample/batch now...\n",
    "\n",
    "    print('Epoch', str(iteration+1).zfill(2), ', Loss:',  loss/(max_pos_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "skip_gram_model = get_keras_cbow(30, len(dic))\n",
    "skip_gram_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 , Loss: 9.00579966771\n",
      "Epoch 02 , Loss: 8.56460475776\n",
      "Epoch 03 , Loss: 8.54432356923\n",
      "Epoch 04 , Loss: 9.12645080861\n",
      "Epoch 05 , Loss: 8.93985762744\n",
      "Epoch 06 , Loss: 9.05797784471\n",
      "Epoch 07 , Loss: 8.90643811885\n",
      "Epoch 08 , Loss: 9.01245992367\n",
      "Epoch 09 , Loss: 8.93351202442\n",
      "Epoch 10 , Loss: 9.06853838378\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "half_window = 2\n",
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "max_pos_start = len(corpus_tokens)-2*half_window\n",
    "\n",
    "for iteration in range(epochs): #iterate the corpus epochs times\n",
    "    loss = 0.\n",
    "    \n",
    "    for pos_start in range(max_pos_start): # iterate on the token positions\n",
    "        x, y = generate_skip_gram_example(pos_start, half_window, corpus_tokens) # generate training examples\n",
    "        \n",
    "        x = one_hot_encoded(x, dic).T  \n",
    "        y = np.array([one_hot_encoded(i, dic) for i in y])\n",
    "        \n",
    "        for i in y: # iterate on the target words\n",
    "            loss += skip_gram_model.train_on_batch(x, i.T) # train the model. only 1 sample/batch now...\n",
    "\n",
    "    print('Epoch', str(iteration+1).zfill(2), ', Loss:',  loss/(max_pos_start*len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
