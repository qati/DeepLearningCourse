{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "**In case of any questions email us: deeplearninginsciences@gmail.com**  \n",
    "**Author: Bálint Ármin Pataki**\n",
    "\n",
    "In this notebook we will create a model that can find a meaningful vector representation of words based on a large text file (corpus). This is an unsupervised learning task, as we do not have any labels (like we had for the happy-sad pictures). We have only the raw text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load numpy\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **toy corpus** (copied from https://en.wikipedia.org/wiki/Text_corpus) we will use is the following:\n",
    "\n",
    "_A corpus may contain texts in a single language (monolingual corpus) or text data in multiple languages (multilingual corpus).\n",
    "Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora. There are two main types of parallel corpora which contain texts in two languages. In a translation corpus, the texts in one language are translations of texts in the other language. In a comparable corpus, the texts are of the same kind and cover the same content, but they are not translations of each other. To exploit a parallel text, some kind of text alignment identifying equivalent text segments (phrases or sentences) is a prerequisite for analysis. Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus.\n",
    "In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation. An example of annotating a corpus is part-of-speech tagging, or POS-tagging, in which information about each word's part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags. Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual._\n",
    "\n",
    "Our **example sentence** will be: _The quick brown fox jumps over the lazy dog_\n",
    "\n",
    "#### Let's save it to a python variable as a string!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = 'A corpus may contain texts in a single language (monolingual corpus) or text data in multiple languages (multilingual corpus). Multilingual corpora that have been specially formatted for side-by-side comparison are called aligned parallel corpora. There are two main types of parallel corpora which contain texts in two languages. In a translation corpus, the texts in one language are translations of texts in the other language. In a comparable corpus, the texts are of the same kind and cover the same content, but they are not translations of each other. To exploit a parallel text, some kind of text alignment identifying equivalent text segments (phrases or sentences) is a prerequisite for analysis. Machine translation algorithms for translating between two languages are often trained using parallel fragments comprising a first language corpus and a second language corpus which is an element-for-element translation of the first language corpus. In order to make the corpora more useful for doing linguistic research, they are often subjected to a process known as annotation. An example of annotating a corpus is part-of-speech tagging, or POS-tagging, in which information about each word\\'s part of speech (verb, noun, adjective, etc.) is added to the corpus in the form of tags. Another example is indicating the lemma (base) form of each word. When the language of the corpus is not a working language of the researchers who use it, interlinear glossing is used to make the annotation bilingual.'\n",
    "example_sentece = 'The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process when the input text (corpus) is splitted into smaller parts, called tokens. Token and word have often similar meaning, but not necessarily.\n",
    "\n",
    "\n",
    "#### For example (the text in the parenthesis is one token):   \n",
    " - _The quick brown fox jumps over the lazy dog._  \n",
    " \n",
    "will be converted to:  \n",
    " - (The), (quick), (brown), (fox), (jumps), (over), (the), (lazy), (dog)\n",
    "\n",
    "**But is some case it is not completely clear:**  \n",
    " - _It's not clear._ \n",
    " \n",
    "can be:\n",
    " - (It's), (not), (clear)\n",
    " - (It), (s), (not), (clear)\n",
    " - (It), ('s), (not), (clear)\n",
    "\n",
    "\n",
    "We will implement a tokenization when we split text an all non alphanumeric characters!  \n",
    "So we pick:\n",
    " - (It), (s), (not), (clear)\n",
    " \n",
    "## Let's implement a tokenizer!\n",
    "### Help:\n",
    "#### get_non_alphanumeric_chars\n",
    " 1. with the `list()` function it is easy to convert the input to list of characters\n",
    " 2. with the `.isalnum()` function you can filter out all the non alphanumeric characters\n",
    " 3. `list(set(duplicated_list))` will keep only unique values in the list\n",
    " \n",
    " \n",
    "#### tokenizer \n",
    " 1. replace all non alphanumeric characters to whitespace! Use `.replace()` function.\n",
    " 2. do the splitting via the `.split()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_non_alphanumeric_chars(input_text):\n",
    "    \"\"\"\n",
    "        Returns the non alphanumeric unique characters from a corpus.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * non_alphanumeric_unique: list of non alphanumeric characters\n",
    "    \"\"\"     \n",
    "    ###Start code here\n",
    "    \n",
    "    #STEP1 set character_list variable to a list of characters in the input_text \n",
    "    character_list = list(input_text)\n",
    "    \n",
    "    #STEP2 store all non_alphanumeric characters from the character_list!\n",
    "    non_alphanumeric = []\n",
    "    for c in character_list:\n",
    "        if c.isalnum() == False:\n",
    "            non_alphanumeric.append(c)\n",
    "    \n",
    "    #STEP3 keep only the unique characters (hint: unique_values = list(set(duplicated_values))\n",
    "    non_alphanumeric_unique = list(set(non_alphanumeric))\n",
    "    \n",
    "    ###End code here \n",
    "    \n",
    "    return non_alphanumeric_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', ' ', \"'\", ',', '.', '?']\n"
     ]
    }
   ],
   "source": [
    "print(get_non_alphanumeric_chars('Hi! How are you? I\\'m fine, thanks... Bye'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', \"'\", ')', '(', '-', ',', '.']\n",
      "[' ', '$', ')', '(', '/', '=']\n"
     ]
    }
   ],
   "source": [
    "print(get_non_alphanumeric_chars(corpus))\n",
    "print(get_non_alphanumeric_chars('bla$$=()/ma z'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output** (maybe you will have different order but that is fine):\n",
    "\n",
    "<pre>['.', ',', '-', \"'\", '(', ' ', ')']  \n",
    "['/', '=', ' ', '(', '$', ')']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def tokenizer(input_text):\n",
    "    \"\"\"\n",
    "        Transforms strings to tokens. The tokens should appear in the same order as they are in the string.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * tokens: list of tokens\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    ###Start code here\n",
    "    #STEP0 extract non alphanumeric characters! Use your function above!\n",
    "    non_alphanumeric_unique = get_non_alphanumeric_chars(input_text)\n",
    "    #print non_alphanumeric_unique\n",
    "    #STEP1 replace all non alphanumeric characters to ' ' whitespace.\n",
    "    replaced_input = input_text\n",
    "    for c in non_alphanumeric_unique:\n",
    "        replaced_input = replaced_input.replace(c, \" \")\n",
    "    #print replaced_input\n",
    "    #STEP2 split the input_text on all the non_alphanumeric_unique characters and store then in the tokens variable\n",
    "    tokens = replaced_input.split()\n",
    "    \n",
    "    ###End code here    \n",
    "    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monolingual', 'corpus', 'or', 'text', 'data', 'in', 'multiple', 'languages', 'multilingual', 'corpus', 'Multilingual', 'corpora', 'that']\n",
      "()\n",
      "['Hi', 'How', 'are', 'you', 'I', 'm', 'fine', 'thanks', 'Bye']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(corpus[49:153]))\n",
    "print()\n",
    "print(tokenizer('Hi! How are you? I\\'m fine, thanks... Bye'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "<pre>\n",
    "['monolingual', 'corpus', 'or', 'text', 'data', 'in', 'multiple', 'languages', 'multilingual', 'corpus', 'Multilingual', 'corpora', 'that']<br>\n",
    "['Hi', 'How', 'are', 'you', 'I', 'm', 'fine', 'thanks', 'Bye']\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Stemming\n",
    "This is the process when the inflected words are converted to their root word.\n",
    "\n",
    "For example: goes $\\to$ go, stemming $\\to$ stem, fishing $\\to$ fish\n",
    "\n",
    "This can help a lot when the corpus is not large enough because then we do not have that many occurence of a word. And having just an _s_ at the end of the word means a completely different word as we will see below.\n",
    "\n",
    "\n",
    "It is not that easy to write a proper stemmer, so we will **skip** this part in the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dictionary & one-hot encoding\n",
    "\n",
    "When the tokenization is ready we can build up a dictionary of the tokens. This is simply the sorted version of the tokens.\n",
    "\n",
    "A dictionary looks like: ['a', 'an', 'apple', ..., 'orange', ..., 'zebra', 'zulu']\n",
    "\n",
    "From now we can think of a word as a vector which has 1 at the position of the word in the dictionary, and 0-s anywhere else.\n",
    "\n",
    "$a = \\begin{bmatrix}\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\\:\\:$ \n",
    "$an = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\\:\\:$\n",
    "$apple = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           1 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           0\n",
    "         \\end{bmatrix}\\:\\: ... \\:\\:$\n",
    "$zebra = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           1 \\\\\n",
    "           0\n",
    "         \\end{bmatrix},\\:\\:$\n",
    "$zulu = \\begin{bmatrix}\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0 \\\\\n",
    "           1\n",
    "         \\end{bmatrix}$\n",
    "         \n",
    "         \n",
    "### Let's build a dictionary and the one-hot encoding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_dictionary(input_text):\n",
    "    \"\"\"\n",
    "        Builds up the dictionary from an input text.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * dictionary: list of unique, sorted tokens\n",
    "    \"\"\"    \n",
    "    tokens = tokenizer(input_text)\n",
    "    ###Start code here\n",
    "    \n",
    "    unique_tokens = tokenizer(input_text)\n",
    "    unique_sorted_tokens = sorted(list(set(unique_tokens)))\n",
    "    \n",
    "    ###End code here\n",
    "    dictionary = unique_sorted_tokens\n",
    "\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'added', 'adjective', 'algorithms', 'aligned', 'alignment', 'an', 'analysis', 'and']\n",
      "118\n"
     ]
    }
   ],
   "source": [
    "print(get_dictionary(corpus.lower())[0:10])\n",
    "print(len(get_dictionary(corpus.lower())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "<pre>\n",
    "['a', 'about', 'added', 'adjective', 'algorithms', 'aligned', 'alignment', 'an', 'analysis', 'and']  \n",
    "118\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def one_hot_encoded(word, dictionary):\n",
    "    \"\"\"\n",
    "        Turn a word to a one-hot encoded vector based on the dictionary.\n",
    "        Input: \n",
    "            * word:   string\n",
    "            * dictionary: list of words in a dictionary\n",
    "        Output:\n",
    "            * oh_vec: one-hot encoded vector\n",
    "    \"\"\"    \n",
    "    #check if word is in dictionary\n",
    "    if(not word in dictionary):\n",
    "        print('The word _' + word + '_ is not in dictionary!')\n",
    "        return None\n",
    "    \n",
    "    ###Start code here\n",
    "    # oh_vec should be a numpy array with a shape (len(dictionary), 1)\n",
    "    # it should contains 0 everywhere except for the position of the word in the dictionary. There is should be 1. \n",
    "    oh_vec = np.zeros((len(dictionary), 1), dtype=int)\n",
    "    #print oh_vec\n",
    "    oh_vec[dictionary.index(word)][0] = 1\n",
    "    ###End code here\n",
    "\n",
    "    return oh_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "()\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n",
      "()\n",
      "The word _ablak_ is not in dictionary!\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "\n",
    "print(one_hot_encoded('a', dic)[0:10])\n",
    "print()\n",
    "print(one_hot_encoded('aligned', dic)[0:10])\n",
    "print()\n",
    "print(one_hot_encoded('ablak', dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "<pre>\n",
    "[[1]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]]\n",
    "\n",
    "[[0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [1]\n",
    " [0]\n",
    " [0]\n",
    " [0]\n",
    " [0]]\n",
    "\n",
    "The word _ablak_ is not in dictionary!\n",
    "None\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Continuous bag of words (CBOW) concept\n",
    "\n",
    "In the CBOW model we try to fit a model that can predict a word by it's neighbors. We expect that if we manage to fit such a model, the inner representation of the model will capture the meaning of the different words.  \n",
    "\n",
    "The number of neighbors is a parameter, for 4 neighbors it looks like (the **red is the target** word, the blue words are the input, the neighbors):  \n",
    "<font color=\"blue\">The quick</font> <font color=\"red\">brown</font> <font color=\"blue\">fox jumps</font> over the lazy dog  \n",
    "The <font color=\"blue\">quick brown</font> <font color=\"red\">fox</font> <font color=\"blue\">jumps over</font> the lazy dog  \n",
    "The quick <font color=\"blue\">brown fox</font> <font color=\"red\">jumps</font> <font color=\"blue\">over the</font> lazy dog  \n",
    "The quick brown <font color=\"blue\">fox jumps</font> <font color=\"red\">over</font> <font color=\"blue\">the  lazy</font> dog  \n",
    "\n",
    "As it can be seen above from a sentence it is easy to create many training examples!\n",
    "\n",
    "#### Let's make a CBOW example generator!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def generate_CBOW_example(start_pos, half_window, tokens):\n",
    "    \"\"\"\n",
    "        Generated CBOW training examples from tokens\n",
    "        Input: \n",
    "            * start_pos:   the position of the first example\n",
    "            * half_window: number of tokens on one-side\n",
    "            * tokens: list of tokens\n",
    "        Output:\n",
    "            * X: input words (the neighbors)\n",
    "            * Y: target word\n",
    "    \"\"\"     \n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    X = tokens[start_pos:start_pos+half_window] + tokens[start_pos+half_window+1:start_pos+2*half_window+1]\n",
    "    Y = tokens[start_pos+half_window]\n",
    "    \n",
    "    ###End code here\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['a', 'corpus', 'contain', 'texts'], 'may')\n",
      "(['may', 'contain', 'in', 'a'], 'texts')\n",
      "(['a', 'corpus', 'may', 'texts', 'in', 'a'], 'contain')\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "\n",
    "print(generate_CBOW_example(0, 2, corpus_tokens))\n",
    "print(generate_CBOW_example(2, 2, corpus_tokens))\n",
    "print(generate_CBOW_example(0, 3, corpus_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "<pre>\n",
    "(['a', 'corpus', 'contain', 'texts'], 'may')\n",
    "(['may', 'contain', 'in', 'a'], 'texts')\n",
    "(['a', 'corpus', 'may', 'texts', 'in', 'a'], 'contain')\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Skip-gram concept\n",
    "\n",
    "Skip-gram is the opposite of the CBOW. Now starting from a single word we want to predict it's neighbors.\n",
    "\n",
    "In the skip-gram model we try to fit a model that can predict a word's neighbors. We expect that if we manage to fit such a model, the inner representation of the model will capture the meaning of the different words.  \n",
    "\n",
    "The number of neighbors is a parameter, for 4 neighbors it looks like (the **blue are the target** words, the red word is the input):  \n",
    "<font color=\"blue\">The quick</font> <font color=\"red\">brown</font> <font color=\"blue\">fox jumps</font> over the lazy dog  \n",
    "The <font color=\"blue\">quick brown</font> <font color=\"red\">fox</font> <font color=\"blue\">jumps over</font> the lazy dog  \n",
    "The quick <font color=\"blue\">brown fox</font> <font color=\"red\">jumps</font> <font color=\"blue\">over the</font> lazy dog  \n",
    "The quick brown <font color=\"blue\">fox jumps</font> <font color=\"red\">over</font> <font color=\"blue\">the  lazy</font> dog  \n",
    "\n",
    "As it can be seen above from a sentence it is easy to create many training examples!\n",
    "\n",
    "#### Let's make a skip-gram example generator!\n",
    "#### Luckily it is very easy now. We only need to change X and Y in the CBOW generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_skip_gram_example(start_pos, half_window, tokens):\n",
    "    \"\"\"\n",
    "        Generated skip-gram training examples from tokens\n",
    "        Input: \n",
    "            * start_pos:   the position of the first example\n",
    "            * half_window: number of tokens on one-side\n",
    "            * tokens: list of tokens\n",
    "        Output:\n",
    "            * x: input word\n",
    "            * y: target words\n",
    "    \"\"\"     \n",
    "    \n",
    "    X, Y = generate_CBOW_example(start_pos, half_window, tokens)\n",
    "    x = Y\n",
    "    y = X\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Keras models\n",
    "\n",
    "Skip-gram is said to word better for smaller amount of training data and for rare words, but CBOW is faster to train and said to be better for frequent words.\n",
    "\n",
    "If we have a corpus with $N$ tokens then we will have $N$ training examples for CBOW and $\\approx window\\_size \\cdot N$ training examples for skip-gram. \n",
    "\n",
    "Training these models can be really slow so a few other tricks are used for training. For the simplicity we won't implement them now.\n",
    "\n",
    "## 6.1 CBOW model\n",
    "\n",
    "In the model we take a training example which contains $N$ input words and a target word. \n",
    "\n",
    "<img src='images/cbow.png'></img>\n",
    "<center>[Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space, 2013]</center>\n",
    "\n",
    "The model is the following:\n",
    " 1. convert the $N$ input words to one-hot encoded ($V$ long) vectors and average them!\n",
    " 2. add a hidden layer of $d$ neurons (no activation, no bias, just the matrix multiplication)\n",
    " 3. add an output layer with $V$ neurons (no bias) and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def get_keras_cbow(hidden_dim, dictionary_size):\n",
    "    \"\"\"\n",
    "        Generate keras model for CBOW model\n",
    "        Input: \n",
    "            * hidden_dim: number of nurons in the hidden layer\n",
    "            * dictionary_size: length of the dictionary (output/input size)\n",
    "        Output:\n",
    "            * model: keras model that implements CBOW \n",
    "    \"\"\"\n",
    "    cbow = Sequential()\n",
    "    ###Start code here\n",
    "    \n",
    "    # add a Dense layer with hidden_dim neurons. Input_dim is the dictionary_size \n",
    "    # we don't need nor activation neither bias!\n",
    "    cbow.add(Dense(hidden_dim,input_dim=dictionary_size))\n",
    "    \n",
    "    # add a Dense layer with dictionary_size neurons. We don't need bias. Activation is softmax.\n",
    "    cbow.add(Dense(dictionary_size, activation='softmax'))\n",
    "    \n",
    "    ###End code here\n",
    "    \n",
    "    return cbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's train it on out toy corpus. We should see the loss decreasing, but we do not expect anything serious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "cbow_model = get_keras_cbow(30, len(dic))\n",
    "cbow_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', '01', ', Loss:', 4.76769242109346)\n",
      "('Epoch', '02', ', Loss:', 4.547351667703676)\n",
      "('Epoch', '03', ', Loss:', 4.286773908236795)\n",
      "('Epoch', '04', ', Loss:', 3.997391689907421)\n",
      "('Epoch', '05', ', Loss:', 3.7402127306323405)\n",
      "('Epoch', '06', ', Loss:', 3.4930301293853887)\n",
      "('Epoch', '07', ', Loss:', 3.2343938774313807)\n",
      "('Epoch', '08', ', Loss:', 2.965933325369496)\n",
      "('Epoch', '09', ', Loss:', 2.69564057184645)\n",
      "('Epoch', '10', ', Loss:', 2.432002490955936)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "half_window = 2\n",
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "max_pos_start = len(corpus_tokens)-2*half_window\n",
    "\n",
    "for iteration in range(epochs): #iterate the corpus epochs times\n",
    "    loss = 0.\n",
    "    \n",
    "    for pos_start in range(max_pos_start): # iterate on the token positions\n",
    "        x, y = generate_CBOW_example(pos_start, half_window, corpus_tokens) # generate training examples\n",
    "        \n",
    "        x = np.array([one_hot_encoded(i, dic) for i in x]).sum(0).T \n",
    "        y = one_hot_encoded(y, dic).T\n",
    "        \n",
    "        loss += cbow_model.train_on_batch(x, y) # train the model. only 1 sample/batch now...\n",
    "\n",
    "    print('Epoch', str(iteration+1).zfill(2), ', Loss:',  loss/(max_pos_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the loss decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 skip-gram model\n",
    "\n",
    "In the model we take a training example which contains an input word and the $N$ target words. We will handle the target words separately, so actually it means $N$ different training X-Y pairs. \n",
    "\n",
    "\n",
    "<img src='images/skip_gram.png'></img>\n",
    "<center>[Mikolov: Efficient Estimation of Word Representations in\n",
    "Vector Space, 2013]</center>\n",
    "\n",
    "The model is the following:\n",
    " 1. convert the input word and one of the target word to one-hot encoded ($V$ long) vector!\n",
    " 2. add a hidden layer of $d$ neurons (no activation, no bias, just the matrix multiplication)\n",
    " 3. add an output layer with $V$ neurons (no bias) and softmax activation.\n",
    " \n",
    "Luckily it is exactly the same model than the CBOW. The only difference is how we create the input and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dic = get_dictionary(corpus.lower())\n",
    "skip_gram_model = get_keras_cbow(30, len(dic))\n",
    "skip_gram_model.compile(optimizer='adam', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', '01', ', Loss:', 4.752132970439501)\n",
      "('Epoch', '02', ', Loss:', 4.409144571251121)\n",
      "('Epoch', '03', ', Loss:', 4.230753688772848)\n",
      "('Epoch', '04', ', Loss:', 4.116837238723582)\n",
      "('Epoch', '05', ', Loss:', 3.9940485673502457)\n",
      "('Epoch', '06', ', Loss:', 3.8593641066354167)\n",
      "('Epoch', '07', ', Loss:', 3.718078181763326)\n",
      "('Epoch', '08', ', Loss:', 3.5770511219570458)\n",
      "('Epoch', '09', ', Loss:', 3.4424605907733774)\n",
      "('Epoch', '10', ', Loss:', 3.3186080837545315)\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "half_window = 2\n",
    "corpus_tokens = tokenizer(corpus.lower())\n",
    "max_pos_start = len(corpus_tokens)-2*half_window\n",
    "\n",
    "for iteration in range(epochs): #iterate the corpus epochs times\n",
    "    loss = 0.\n",
    "    \n",
    "    for pos_start in range(max_pos_start): # iterate on the token positions\n",
    "        x, y = generate_skip_gram_example(pos_start, half_window, corpus_tokens) # generate training examples\n",
    "        \n",
    "        x = one_hot_encoded(x, dic).T  \n",
    "        y = np.array([one_hot_encoded(i, dic) for i in y])\n",
    "        \n",
    "        for i in y: # iterate on the target words\n",
    "            loss += skip_gram_model.train_on_batch(x, i.T) # train the model. only 1 sample/batch now...\n",
    "\n",
    "    print('Epoch', str(iteration+1).zfill(2), ', Loss:',  loss/(max_pos_start*len(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the loss decreasing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is the end of the homework. The deadline is 2018.04.24.\n",
    "\n",
    "In the next homework we will continue to our smiley generator!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
