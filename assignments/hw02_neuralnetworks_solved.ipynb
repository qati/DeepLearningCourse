{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "def accuracy(Y,P):\n",
    "    return np.count_nonzero(np.equal(np.argmax(Y,axis=-1),np.argmax(P,axis=-1)).astype(int))/float(Y.shape[0])\n",
    "\n",
    "def visualize(data):\n",
    "    p = data.reshape((28,28))\n",
    "    plt.imshow(p,interpolation='none', cmap='gray_r')\n",
    "    plt.show()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], x_train.shape[1]*x_train.shape[2]))/255.\n",
    "x_test = x_test.reshape((x_test.shape[0], x_test.shape[1]*x_test.shape[2]))/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(y_train[0:10])\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2413183c080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def one_hot(y):\n",
    "    \"\"\"\n",
    "        Transforms labels to one-hot encoded labels.\n",
    "        Input: \n",
    "            * y:   (M,)\n",
    "        Output:\n",
    "            * y_oh: (M, K)\n",
    "    \"\"\"\n",
    "    y_oh = None\n",
    "    ###Start code here\n",
    "    \n",
    "    M = len(y)\n",
    "    K = np.max(y) + 1\n",
    "    y_oh = np.zeros((M,K))\n",
    "    for i in range(M):\n",
    "        y_oh[i,y[i]] = 1\n",
    "    \n",
    "    ###End code here\n",
    "    return y_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(y_train)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "y_train_oh = one_hot(y_train)\n",
    "y_test_oh = one_hot(y_test)\n",
    "\n",
    "print(y_train_oh.shape)\n",
    "print(y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def initialize_weights(layer_dimensions):\n",
    "    \"\"\"\n",
    "        Initialize the weights of the neural network.\n",
    "        Input:\n",
    "            * layer_dimensions: list containing the number of neurons for each layer. \n",
    "                                layer_dimensions[0] is the dimensionality of our data. \n",
    "                                layer_dimensions[1] is the number of neurons in the first layer.\n",
    "        Output:\n",
    "            * parameters: dict containing the weights of the network.\n",
    "                          parameters[\"W1\"] = the W weight matrix of the first layer\n",
    "                          parameters[\"b1\"] = the b bias node of the first layer\n",
    "    \"\"\"\n",
    "    parameters={}\n",
    "    ###Start code here\n",
    "    \n",
    "    for i in range (len(layer_dimensions)-1):\n",
    "        weight_name = \"W\" + str(i+1)\n",
    "        bias_name = \"b\" + str(i+1)\n",
    "        \n",
    "        parameters[weight_name] = np.random.randn(layer_dimensions[i+1],layer_dimensions[i])*0.01\n",
    "        parameters[bias_name] = np.zeros((layer_dimensions[i+1],1))\n",
    "\n",
    "    ###End code here\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1.shape= (6, 3)\n",
      "b1.shape= (6, 1)\n",
      "W1= [[ 0.01764052  0.00400157  0.00978738]\n",
      " [ 0.02240893  0.01867558 -0.00977278]\n",
      " [ 0.00950088 -0.00151357 -0.00103219]\n",
      " [ 0.00410599  0.00144044  0.01454274]\n",
      " [ 0.00761038  0.00121675  0.00443863]\n",
      " [ 0.00333674  0.01494079 -0.00205158]]\n",
      "b1= [[ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]\n",
      " [ 0.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "parameters = initialize_weights([3,6])\n",
    "print(\"W1.shape=\",parameters[\"W1\"].shape)\n",
    "print(\"b1.shape=\",parameters[\"b1\"].shape)\n",
    "print(\"W1=\",parameters[\"W1\"])\n",
    "print(\"b1=\",parameters[\"b1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def linear_forward(W, b, a_prev):\n",
    "    \"\"\"\n",
    "        Linear unit in the computational graph.\n",
    "        Inputs:\n",
    "                * W: the weight matrix of the unit (dimensions in the graph above)\n",
    "                * b: the bias vector of the unit (dimensions in the graph above)\n",
    "                * a_prev: the activation from the previous node (dimensions in the graph above)\n",
    "        Outputs:\n",
    "                * Z:     the result (dimensions in the graph above)\n",
    "                * cache: combination of values you will need in backward passes\n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "    \n",
    "    # Calculate Z\n",
    "    \n",
    "    Z = np.dot(W,a_prev) + b\n",
    "\n",
    "    # Create a cache for backpropagation\n",
    "    cache = (Z,W,a_prev)\n",
    "    \n",
    "    ###End code here\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01514415]\n",
      " [ 0.01947252]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "parameters = initialize_weights([3,2])\n",
    "x = np.random.randn(1,3).T\n",
    "Z, cache = linear_forward(parameters[\"W1\"],parameters[\"b1\"],x)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def activation_forward(Z):\n",
    "    \"\"\"\n",
    "        Activation node on the forward computational graph.\n",
    "        Inputs:\n",
    "                Z: the ouput of linear unit\n",
    "        Outputs:\n",
    "                a: the activation\n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "   \n",
    "    #implement sigmoid activation\n",
    "    a = 1./(1+np.exp(-1*Z))\n",
    "\n",
    "    ###End code here\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.85371646,  0.59872543,  0.72685773],\n",
       "       [ 0.9038621 ,  0.86617546,  0.27343225]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Z = np.random.randn(2,3)\n",
    "activation_forward(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "        Calculates the softmax of a matrix.\n",
    "        Input: \n",
    "            * Z: (M,K)=(num_examples, num_categories)\n",
    "        Returns:\n",
    "            * a: (M,K)=(num_examples, num_categories)\n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "    \n",
    "    # Transform Z to probabilites\n",
    "    # The sum should be on categories!\n",
    "\n",
    "    a = np.divide(np.exp(Z),np.sum(np.exp(Z),axis = 1, keepdims = True))\n",
    "    \n",
    "    ###End code here\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58423523,  0.14936733,  0.26639744],\n",
       "       [ 0.57854881,  0.39829292,  0.02315827]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Z = np.random.randn(2,3)\n",
    "softmax(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_forward(parameters, X):\n",
    "    \"\"\"\n",
    "        Calculates a forward step in the network.\n",
    "        Input:\n",
    "            * parameters: dict of parameters.\n",
    "                          parameters[\"W1\"]= the W matrix of first layer\n",
    "                          parameters[\"b4\"]=the bias vector in layer 4\n",
    "            * X: input matrix. Shape (M,K)\n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    # Set a_prev to the input!\n",
    "    # In the first node of the computational graph, the input is transposed!\n",
    "    a = X.T\n",
    "    \n",
    "    # Go trough the nodes of the computational graph, from LEFT to RIGHT\n",
    "    # Always use as the input of current node the ouput of previous node!\n",
    "    # Get the W, b weights from the parameters dictionary\n",
    "    # use linear_forward and activation_forward function to step one in the layer\n",
    "    # append the linear unit's cache to caches list\n",
    "    \n",
    "    for i in range(L-1):\n",
    "        weight_name = \"W\" + str(i+1)\n",
    "        bias_name = \"b\" + str(i+1)\n",
    "        \n",
    "        Z,new_cache = linear_forward(parameters[weight_name],parameters[bias_name],a)\n",
    "        caches.append(new_cache)\n",
    "        \n",
    "        a = activation_forward(Z)\n",
    "    \n",
    "    # Important: the last layer activation should be softmax! \n",
    "    # Important: Transpose Z (in the last unit)\n",
    "            \n",
    "    weight_name = \"W\" + str(L)\n",
    "    bias_name = \"b\" + str(L)\n",
    "        \n",
    "    Z,new_cache = linear_forward(parameters[weight_name],parameters[bias_name],a)\n",
    "    caches.append(new_cache)\n",
    "    \n",
    "    a = softmax(Z.T)\n",
    "\n",
    "    ###End code here\n",
    "    \n",
    "    return (a,caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.49967587  0.50032413]\n",
      " [ 0.49966831  0.50033169]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,4,2])\n",
    "x = np.random.randn(2,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "print(AL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def crossentropy(P, Y):\n",
    "    \"\"\"\n",
    "        Calculates the crossentropy-loss shown in the computational graph above.\n",
    "        Inputs:\n",
    "                * P the prediction of the network (dimensions above)\n",
    "                * Y the one-hot encoded labels\n",
    "        Outputs:\n",
    "                * The crossentropy between the P and Y\n",
    "    \"\"\"\n",
    "    ### Start code here\n",
    "   \n",
    "    #calculate the coross entropy loss\n",
    "    \n",
    "    M = len(Y)\n",
    "    loss = -1/M * np.sum(np.multiply(Y,np.log(P)))\n",
    "    \n",
    "    ### End code here\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69315495522680215"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,4,2])\n",
    "x = np.random.randn(2,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "y = np.array(([[0,1],[1,0]]))\n",
    "crossentropy(AL,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def losssoftmax_backward(P,Y):\n",
    "    \"\"\"\n",
    "        Calculates the dLdZ derivate which is in the top of the network. \n",
    "        Inputs:\n",
    "                * P the prediction of the network\n",
    "                * Y the labels\n",
    "        Outputs:\n",
    "                * dLdZ the derivate of the L loss function \n",
    "    \"\"\"\n",
    "    ###Start code here\n",
    "    \n",
    "    #implement the calculation shown in the graph\n",
    "    \n",
    "    M = len(Y)\n",
    "    dLdZ = 1/M * (P-Y).T\n",
    "    \n",
    "    ###End code here\n",
    "    return dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16655862, -0.16677723, -0.16676625],\n",
       "       [-0.16655862,  0.16677723,  0.16676625]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,4,2])\n",
    "x = np.random.randn(3,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "y = np.array(([[0,1],[1,0],[1,0]]))\n",
    "\n",
    "losssoftmax_backward(AL,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def activation_backward(dLda, cache):\n",
    "    \"\"\"\n",
    "        Calculates the backward step in a non-linearity.\n",
    "        Inputs:\n",
    "            * dLda: The derivates above the current unit.\n",
    "            * cache: The cache from the forward step.\n",
    "    \"\"\"\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    # Use the cache from the foward step\n",
    "    # Calculate the g'(Z): the sigmoid derivate (the derivate is: sigmoid(Z)*(1-sigmoid(Z)) )\n",
    "    # Do the calculation shown in the graph.\n",
    "    \n",
    "    Z = cache[0]\n",
    "    g_prime = np.exp(Z)/np.multiply(1+np.exp(Z),1+np.exp(Z))\n",
    "    dLdZ = np.multiply(dLda, g_prime)\n",
    "    \n",
    "    ###End code here\n",
    "    return dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01689878, -0.2133506 ],\n",
       "       [-0.06251018,  0.10071241],\n",
       "       [-0.14419048, -0.03281763],\n",
       "       [-0.08698937, -0.18342292]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "Z      = np.random.randn(4,2)\n",
    "W      = np.random.randn(4,3)\n",
    "a_prev = np.random.randn(3,2)\n",
    "dLda   = np.random.randn(4,2)\n",
    "    \n",
    "\n",
    "dLdZ = activation_backward(dLda, (Z, W, a_prev))\n",
    "\n",
    "dLdZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def linear_backward(dLdZ, cache):\n",
    "    \"\"\"\n",
    "        Implements the backward step on linear unit as shown in the graph above/\n",
    "        Inputs: \n",
    "                * dLdZ:  the derivate from the next node\n",
    "                * cache: the cache form froward step\n",
    "        Returns:\n",
    "                * dLda\n",
    "                * dLdW\n",
    "                * dLdb\n",
    "    \"\"\"\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    # Use the cache from the forward step\n",
    "    W = cache[1]\n",
    "    a = cache[2]\n",
    "    #Calculate dLda as shown in the graph\n",
    "    dLda = np.dot(W.T, dLdZ)\n",
    "    #Calculate the dLdW shown in the graph\n",
    "    dLdW = np.dot(dLdZ, a.T)\n",
    "    #Calculate the dLdb shown in the graph\n",
    "    # Use keepdims=True\n",
    "    dLdb = np.sum(dLdZ,1,keepdims=True)\n",
    "    \n",
    "    ###End code here\n",
    "    return (dLda, dLdW, dLdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dLda= [[ 0.00238192 -0.00237095 -0.00237084]\n",
      " [-0.00085508  0.00085114  0.0008511 ]]\n",
      "dLdW= [[-0.08095971 -0.08519952]\n",
      " [ 0.08095971  0.08519952]]\n",
      "dLdb= [[-0.16551512]\n",
      " [ 0.16551512]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,2,2])\n",
    "x = np.random.randn(3,3)\n",
    "AL, cache = nn_forward(parameters, x)\n",
    "y = np.array(([[0,1],[1,0],[1,0]]))\n",
    "\n",
    "dLdZ=losssoftmax_backward(AL,y)\n",
    "\n",
    "dLda, dLdW, dLdb = linear_backward(dLdZ, cache[-1])\n",
    "print(\"dLda=\",dLda)\n",
    "print(\"dLdW=\",dLdW)\n",
    "print(\"dLdb=\",dLdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_backward(X, Y, P, caches):\n",
    "    \"\"\"\n",
    "        Calculates the backward pass for the network as shown in the computational graph above.\n",
    "        Inputs:\n",
    "                * X: The input examples: (M,N) = (number_of_examples, dimensionality_of_data)\n",
    "                * Y: The one-hot encoded labels: (M,K) = (number_of_examples, number_of_categories)\n",
    "                * P: The network prediction: (M,K)\n",
    "                * caches: the cache from forward step\n",
    "        Returns:\n",
    "                * derivates: dictionary containing all the derivates\n",
    "                             derivates[\"dLdW2\"] = the derivate of the loss L regarding to the weight matrix W of 2. layer\n",
    "    \"\"\"\n",
    "    derivates = {}\n",
    "    ###Start code here\n",
    "    \n",
    "    # Implement the calculation shown in the graph!\n",
    "    # The arrows in the graph: the output of a unit is the input of the previous unit\n",
    "    # IMPORTANT: you have to go trough the graph from RIGHT to LEFT\n",
    "    \n",
    "    # The last node derivative (loss) initializes the calculation by setting the derivative in the top of the network\n",
    "    \n",
    "    NL = len(caches) #number of layers\n",
    "    dLdZ_name = \"dLdZ\" + str(NL)\n",
    "    derivates[dLdZ_name] = losssoftmax_backward(P,Y)\n",
    "    \n",
    "    dLda_name = \"dLda\" + str(NL-1)\n",
    "    dLdW_name = \"dLdW\" + str(NL)\n",
    "    dLdb_name = \"dLdb\" + str(NL)    \n",
    "    (derivates[dLda_name], derivates[dLdW_name], derivates[dLdb_name]) = linear_backward(derivates[dLdZ_name], caches[-1])\n",
    "    \n",
    "    # compute the backward pass on non-linearity (use the function you implemented)\n",
    "    # compute the backward pass on linear unit (use the function you implemented)\n",
    "    # save the derivates to the derivates dict\n",
    "    # derivates[\"dLdW2\"] = the derivate of the loss L regarding to the weight matrix W of 2. layer\n",
    "        \n",
    "    for i in range(NL-1,0,-1):\n",
    "        dLdZ_name = \"dLdZ\" + str(i)\n",
    "        dLdW_name = \"dLdW\" + str(i)\n",
    "        dLdb_name = \"dLdb\" + str(i)\n",
    "        \n",
    "        derivates[dLdZ_name] = activation_backward(derivates[dLda_name], caches[i-1])\n",
    "        dLda_name = \"dLda\" + str(i-1)\n",
    "        (derivates[dLda_name], derivates[dLdW_name], derivates[dLdb_name]) = linear_backward(derivates[dLdZ_name], caches[i-1])\n",
    "        \n",
    "    ###End code here\n",
    "    return derivates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dLdW1': array([[ -6.21487264e-04,  -2.36563074e-04,   1.23496218e-03],\n",
       "        [  3.60961474e-04,  -1.49180228e-05,  -4.73934376e-04]]),\n",
       " 'dLdW2': array([[  1.23509189e-05,  -1.36228318e-03],\n",
       "        [ -2.24388132e-03,   2.57328588e-03]]),\n",
       " 'dLdZ1': array([[-0.00064366, -0.00044455,  0.00064597,  0.00044658],\n",
       "        [ 0.00031537,  0.00015953, -0.00031617, -0.00016025]]),\n",
       " 'dLdZ2': array([[-0.12470383, -0.12471917,  0.12528666,  0.12529123],\n",
       "        [-0.12529617,  0.12471917,  0.12471334, -0.12529123]]),\n",
       " 'dLda0': array([[ -1.38391873e-05,  -8.93266454e-06,   1.38852440e-05,\n",
       "           8.97344753e-06],\n",
       "        [  6.66691944e-06,   4.10011310e-06,  -6.68793679e-06,\n",
       "          -4.11880490e-06],\n",
       "        [ -3.85877054e-06,  -1.32363032e-06,   3.86488992e-06,\n",
       "           1.32951446e-06]]),\n",
       " 'dLda1': array([[-0.00257559, -0.00177821,  0.0025839 ,  0.00178637],\n",
       "        [ 0.00126171,  0.00063836, -0.00126469, -0.00064129]]),\n",
       " 'dLdb1': array([[  4.34323857e-06],\n",
       "        [ -1.51825743e-06]]),\n",
       " 'dLdb2': array([[ 0.00115489],\n",
       "        [-0.00115489]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "parameters = initialize_weights([3,2,2])\n",
    "x = np.random.randn(4,3)\n",
    "y = (np.random.randn(4,2)>0.5).astype(int)\n",
    "AL, caches = nn_forward(parameters, x)\n",
    "\n",
    "nn_backward(x,y, AL, caches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_train_gd(X, Y, layers, n_iter=1000, lr=0.01):\n",
    "    \"\"\"\n",
    "        Creates and trains a neural network with gradient descent.\n",
    "        Inputs:\n",
    "                * X: the images (number_of_examples, 28*28)\n",
    "                * Y: the labels (number_of_examples, 10)\n",
    "                * layers: the definition of neural networks\n",
    "                          [28*28, 100, 10]: two layers, 100 neuron in first layer, and 10 neurons in the last layer\n",
    "                          the first element of this list must be 28*28\n",
    "                          the number of neurons in the last layer must be the same as Y.shape[1]\n",
    "                * n_iter: how many iteration we want with gradient descent\n",
    "                * lr:     learning rate\n",
    "        Returns:\n",
    "                * parameters: the trained parameters of the network\n",
    "                * losses:     the loss values\n",
    "    \"\"\"\n",
    "    L = len(layers)\n",
    "    losses = []\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    #initialize the weights of the network\n",
    "    parameters = initialize_weights(layers)\n",
    "    \n",
    "    # In one iteration of gradient descent:\n",
    "      # Do a forward step in the network. Use nn_forward\n",
    "      # Calculate the derivates doing a backward step in the network. Use nn_backward\n",
    "      # Update the weights\n",
    "      # Calculate the loss. Use crossentropy. (and save it to losses)\n",
    "    \n",
    "    for iteration in range(n_iter):\n",
    "        AL, caches = nn_forward(parameters, X)\n",
    "        losses.append(crossentropy(AL,Y))\n",
    "        dL = nn_backward(X,Y, AL, caches)\n",
    "    \n",
    "        for i in range(L-1):\n",
    "            weight_name = \"W\" +str(i+1)\n",
    "            bias_name = \"b\" + str(i+1)\n",
    "            dLdW_name = \"dLdW\" + str(i+1)\n",
    "            dLdb_name = \"dLdb\" + str(i+1)\n",
    "            parameters[weight_name] = parameters[weight_name] - lr * dL[dLdW_name]\n",
    "            parameters[bias_name] = np.subtract(parameters[bias_name], lr * dL[dLdb_name])\n",
    "   \n",
    "    ###End code here\n",
    "    return parameters,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take a while\n",
    "parameters,losses = nn_train_gd(x_train, y_train_oh, [28*28, 100, 10], n_iter=1, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, _ = nn_forward(parameters, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10441666666666667"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train_oh, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRADED function\n",
    "#Don't change the function name, parameters and return values\n",
    "def nn_train_sgd(X, Y, layers, batch_size=100, epochs=100, lr=0.01):\n",
    "    \"\"\"\n",
    "        Creates and trains a neural network with gradient descent.\n",
    "        Inputs:\n",
    "                * X: the images (number_of_examples, 28*28)\n",
    "                * Y: the labels (number_of_examples, 10)\n",
    "                * layers: the definition of neural networks\n",
    "                          [28*28, 100, 10]: two layers, 100 neuron in first layer, and 10 neurons in the last layer\n",
    "                          the first element of this list must be 28*28\n",
    "                          the number of neurons in the last layer must be the same as Y.shape[1]\n",
    "                * n_iter: how many iteration we want\n",
    "                * lr:     learning rate\n",
    "        Returns:\n",
    "                * parameters: the trained parameters of the network\n",
    "                * losses:     the loss values\n",
    "    \"\"\"\n",
    "   \n",
    "    losses = []\n",
    "    \n",
    "    ###Start code here\n",
    "    \n",
    "    #initialize the weights of the network\n",
    "    parameters = initialize_weights(layers)\n",
    "    L = len(layers)\n",
    "    \n",
    "    # Iterate trough epochs\n",
    "       # Iterate trough batches in dataset\n",
    "          # Do a forward step in the network. Just with the mini-batch! Use nn_forward\n",
    "          # Calculate the derivates doing a backward step in the network. Just for the mini-batch! Use nn_backward\n",
    "          # Update the weights\n",
    "       # Calculate the loss. Use crossentropy. Save to losses.\n",
    "    \n",
    "    N_batch = len(x_train) // batch_size\n",
    "    for epoch in range(epochs):\n",
    "        print(\"epoch: \",epoch)\n",
    "        for batch in range(N_batch):\n",
    "            print(\"batch: \",batch)\n",
    "            training_input = X[batch*batch_size:(batch+1)*batch_size-1,:]\n",
    "            training_output = Y[batch*batch_size:(batch+1)*batch_size-1,:]\n",
    "            AL, caches = nn_forward(parameters, X)\n",
    "            losses.append(crossentropy(AL,Y))\n",
    "            dL = nn_backward(X,Y, AL, caches)\n",
    "            \n",
    "            for i in range(L-1):\n",
    "                weight_name = \"W\" +str(i+1)\n",
    "                bias_name = \"b\" + str(i+1)\n",
    "                dLdW_name = \"dLdW\" + str(i+1)\n",
    "                dLdb_name = \"dLdb\" + str(i+1)\n",
    "                parameters[weight_name] = parameters[weight_name] - lr * dL[dLdW_name]\n",
    "                parameters[bias_name] = np.subtract(parameters[bias_name], lr * dL[dLdb_name])\n",
    "\n",
    "    ###End code here\n",
    "    return parameters,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "batch:  0\n",
      "batch:  1\n",
      "batch:  2\n",
      "batch:  3\n",
      "batch:  4\n",
      "batch:  5\n"
     ]
    }
   ],
   "source": [
    "parameters,losses = nn_train_sgd(x_train, y_train_oh, [28*28, 100, 10], batch_size=10000, epochs=1, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24100086978>]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24100079240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P, _ = nn_forward(parameters, x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10441666666666667"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_train_oh, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1028"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P, _ = nn_forward(parameters, x_test)\n",
    "accuracy(y_test_oh, P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24100076dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network predicts:  7\n"
     ]
    }
   ],
   "source": [
    "i=254\n",
    "visualize(x_test[i])\n",
    "print(\"Network predicts: \", np.argmax(P[i,:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
